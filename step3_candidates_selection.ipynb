{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def dcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    There is a typographical error on the formula referenced in the original definition of this function:\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    log2(i) should be log2(i+1)\n",
    "\n",
    "    The formulas here are derived from\n",
    "    https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Discounted_Cumulative_Gain\n",
    "\n",
    "    The formulas return the same results when r contains only binary values\n",
    "\n",
    "    >>> r = [3, 2, 3, 0, 1, 2]\n",
    "    >>> dcg_at_k(r, 1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 1, method=1)\n",
    "    7.0\n",
    "    >>> print(round(dcg_at_k(r, 2), 16))\n",
    "    4.2618595071429155\n",
    "    >>> print(round(dcg_at_k(r, 2, method=1), 16))\n",
    "    8.892789260714373\n",
    "    >>> print(round(dcg_at_k(r, 6), 16))\n",
    "    6.861126688593502\n",
    "    >>> print(round(dcg_at_k(r, 6, method=1), 16))\n",
    "    13.848263629272981\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy array) in rank order\n",
    "            (first element is the most relevant item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then sum rel_i / log2(i + 1) [not log2(i)]\n",
    "                If 1 then sum (2^rel_i - 1) / log2(i + 1)\n",
    "\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        elif method == 1:\n",
    "            return np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    >>> r = [3, 2, 3, 0, 1, 2]\n",
    "    >>> ndcg_at_k(r, 1)\n",
    "    1.0\n",
    "    >>> ndcg_at_k(r, 1, method=1)\n",
    "    1.0\n",
    "    >>> print(round(ndcg_at_k(r, 2), 16))\n",
    "    0.8710490642551529\n",
    "    >>> print(round(ndcg_at_k(r, 2, method=1), 16))\n",
    "    0.7789412530088334\n",
    "    >>> print(round(ndcg_at_k(r, 6), 16))\n",
    "    0.9608081943360616\n",
    "    >>> print(round(ndcg_at_k(r, 6, method=1), 16))\n",
    "    0.9488107485678984\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy array) in rank order\n",
    "            (first element is the most relevant item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then sum rel_i / log2(i + 1) [not log2(i)]\n",
    "                If 1 then sum (2^rel_i - 1) / log2(i + 1)\n",
    "\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> r = [0, 0, 1]\n",
    "    >>> precision_at_k(r, 1)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 2)\n",
    "    0.0\n",
    "    >>> print(round(precision_at_k(r, 3), 16))\n",
    "    0.3333333333333333\n",
    "    >>> precision_at_k(r, 4)\n",
    "    Traceback (most recent call last):\n",
    "        File \"<stdin>\", line 1, in ?\n",
    "    ValueError: Relevance score length < k\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return np.mean(r)\n",
    "\n",
    "def average_precision(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / len(r)\n",
    "    >>> print(round(sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y]), 16))\n",
    "    0.3916666666666667\n",
    "    >>> print(round(average_precision(r), 16))\n",
    "    0.3916666666666666\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.sum(out) / len(r)\n",
    "\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]]\n",
    "    >>> print(round(mean_average_precision(rs), 16))\n",
    "    0.3916666666666666\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [0]]\n",
    "    >>> print(round(mean_average_precision(rs), 16))\n",
    "    0.1958333333333333\n",
    "\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "def mean_recall(rs):\n",
    "    return np.mean([(np.array(r) == 1).mean() for r in rs])\n",
    "\n",
    "\n",
    "def compute_relevance_vector(actual: List[float], predicted: List[float], k: int):\n",
    "    \"\"\"Высчитывает вектор релевантностей длины K\n",
    "    \"\"\"\n",
    "    relevance = []\n",
    "\n",
    "    if (len(actual) >= len(predicted) >= k) or (\n",
    "            (len(actual) >= k and len(predicted) >= k)):\n",
    "        # Нормальный случай, когда K <= actual <= predicted\n",
    "        # Нормальный случай, когда K <= actual and K <= predicted\n",
    "        end_metric_range = k\n",
    "    elif (len(actual) < len(predicted) and len(actual) < k) and len(predicted) >= k:\n",
    "        # Когда просмотренный вектор меньше, чем предсказанный и @K\n",
    "        end_metric_range = len(actual)\n",
    "    else:\n",
    "        raise ValueError('Непредвиденный случай комбинации входных данных:\\n actual:{} , predicted:{} '.format(\n",
    "            len(actual),\n",
    "            len(predicted)\n",
    "        ))\n",
    "\n",
    "    start_metric_range = 0\n",
    "    for i in range(start_metric_range, end_metric_range):\n",
    "        r = 0\n",
    "\n",
    "        p = predicted[i]\n",
    "\n",
    "        if p in actual:\n",
    "            r = 1\n",
    "        relevance.append(r)\n",
    "\n",
    "    return relevance\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, predict_dict: dict, actual_dict: dict):\n",
    "        self.predict_dict = predict_dict\n",
    "        self.actual_dict = actual_dict\n",
    "\n",
    "    def full_relevance_dict(self, val_user_uids: list, k: int, nonpredicted: bool) -> Dict[int, List[int]]:\n",
    "        relevance_dict = {}\n",
    "\n",
    "        a_a = 0\n",
    "        p_a = 0\n",
    "        p_p = 0\n",
    "\n",
    "        for user_uid in tqdm(list(val_user_uids)):\n",
    "\n",
    "            predicted = self.predict_dict.get(user_uid)\n",
    "            actual = self.actual_dict.get(user_uid)\n",
    "            # print(predicted, actual)\n",
    "            # print()\n",
    "\n",
    "            # relevance = compute_relevance_vector(actual, predicted, k)\n",
    "            # print(user_uid)\n",
    "            # print(len(predicted), len(actual))\n",
    "            # print()\n",
    "            if actual is None or len(actual) == 0:\n",
    "                # print('Просмотры для user_uid={} отсутствует'.format(user_uid))\n",
    "                a_a += 1\n",
    "                continue\n",
    "            elif predicted is None:\n",
    "                p_a += 1\n",
    "\n",
    "                if nonpredicted:\n",
    "                    relevance = [0] * len(actual)\n",
    "                else:\n",
    "                    continue\n",
    "                # raise ValueError('Предсказание для user_uid={} отсутствует'.format(user_uid))\n",
    "                # continue\n",
    "            else:\n",
    "                try:\n",
    "                    relevance = compute_relevance_vector(actual, predicted, k)\n",
    "                    p_p += 1\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "            relevance_dict[user_uid] = relevance\n",
    "\n",
    "        print('Просмотры отсутствуют: {}, '\n",
    "              'Предсказание отсутствует: {}, '\n",
    "              'Предсказано: {}'\n",
    "              ''.format(a_a, p_a, p_p))\n",
    "\n",
    "        return relevance_dict\n",
    "\n",
    "    def relevance_list(self, val_user_uids: list, k: int, nonpredicted: bool) -> List[List[int]]:\n",
    "        relevance_list = []\n",
    "        relevance_dict = self.full_relevance_dict(val_user_uids, k, nonpredicted)\n",
    "\n",
    "        for user_uid, relevance in relevance_dict.items():\n",
    "            relevance_list.append(relevance)\n",
    "\n",
    "        return relevance_list\n",
    "\n",
    "\n",
    "def get_metrics_dict(\n",
    "        predict_dict: Dict,\n",
    "        actual_dict: Dict,\n",
    "        current_predict_user_uids: List[int],\n",
    "        k: int,\n",
    "        nonpredicted: bool\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    metrics_dict = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    e = Evaluator(predict_dict, actual_dict)\n",
    "    relevance_list = e.relevance_list(current_predict_user_uids, k=k, nonpredicted=nonpredicted)\n",
    "\n",
    "    metrics_dict['mean_recall'] = mean_recall(relevance_list)\n",
    "    metrics_dict['map'] = mean_average_precision(relevance_list)\n",
    "    metrics_dict['mean_ndcg'] = (\n",
    "        np.mean([ndcg_at_k(r, min(len(r), k)) for r in relevance_list])\n",
    "    )\n",
    "    metrics_dict = dict(metrics_dict)\n",
    "\n",
    "    ideal_metrics_dict = defaultdict(lambda: defaultdict(list))\n",
    "    # Идеальное ранжирование\n",
    "    new_relevance_list = []\n",
    "    for rel in relevance_list:\n",
    "        try:\n",
    "            if sum(rel) > 0:\n",
    "                len_rel = len(rel)\n",
    "                sum_rel = sum(rel)\n",
    "                rel = [1] * sum_rel\n",
    "                rel.extend([0] * (len_rel - sum_rel))\n",
    "            new_relevance_list.append(rel)\n",
    "        except Exception:\n",
    "            pass\n",
    "        new_relevance_list.append(rel)\n",
    "    relevance_list = new_relevance_list\n",
    "\n",
    "    ideal_metrics_dict['mean_recall'] = mean_recall(relevance_list)\n",
    "    ideal_metrics_dict['map'] = mean_average_precision(relevance_list)\n",
    "    ideal_metrics_dict['mean_ndcg'] = (\n",
    "        np.mean([ndcg_at_k(r, min(len(r), k)) for r in relevance_list])\n",
    "    )\n",
    "    ideal_metrics_dict = dict(ideal_metrics_dict)\n",
    "\n",
    "    return metrics_dict, ideal_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "from implicit.nearest_neighbours import ItemItemRecommender, BM25Recommender, TFIDFRecommender, bm25_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "RAW_DATA_PATH = './raw_data/'\n",
    "DATA_PATH = './data'\n",
    "RANDOM_STATE = 42\n",
    "TOP_N = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clicks = pd.read_hdf(os.path.join(DATA_PATH, 'step1_enriched_train_clicks.h5'), index=None, key='step1')\n",
    "train_likes = pd.read_hdf(os.path.join(DATA_PATH, 'step1_enriched_train_likes.h5'), index=None, key='step1')\n",
    "train_shares = pd.read_hdf(os.path.join(DATA_PATH, 'step1_enriched_train_shares.h5'), index=None, key='step1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_clicks = pd.read_hdf(os.path.join(DATA_PATH, 'step1_val_clicks.h5'), index=None, key='step1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, 'val1_users.json'), 'r') as f:\n",
    "    val1_users = set(json.load(f)['users'])\n",
    "    \n",
    "with open(os.path.join(DATA_PATH, 'val2_users.json'), 'r') as f:\n",
    "    val2_users = set(json.load(f)['users'])\n",
    "    \n",
    "val_users = val1_users | val2_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mapping = pd.read_csv(os.path.join(DATA_PATH, 'step0_user_mapping.csv'))\n",
    "picture_mapping = pd.read_csv(os.path.join(DATA_PATH, 'step0_picture_mapping.csv'))\n",
    "\n",
    "user_mapping_dict = user_mapping.groupby('old')['new'].first().to_dict()\n",
    "picture_mapping_dict = picture_mapping.groupby('old')['new'].first().to_dict()\n",
    "inv_picture_mapping_dict = dict([(v, k)for k, v in picture_mapping_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_users_pictures(_df):\n",
    "    df = _df.copy()\n",
    "    df_user_actions_count = df.groupby('user_id')['picture_id'].count()\n",
    "    selected_users = df_user_actions_count[df_user_actions_count > 1].index.tolist()\n",
    "    df = df[df['user_id'].isin(selected_users)]\n",
    "\n",
    "    df_picture_actions_count = df.groupby('picture_id')['user_id'].count()\n",
    "    selected_pictures = df_picture_actions_count[df_picture_actions_count > 1].index.tolist()\n",
    "    df = df[df['picture_id'].isin(selected_pictures)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1497835, 446189)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cutted_train_clicks = train_clicks.copy()\n",
    "# cutted_train_clicks = filter_users_pictures(cutted_train_clicks)\n",
    "# cutted_train_clicks.shape\n",
    "\n",
    "selected_train_clics = train_clicks\n",
    "\n",
    "user_ids = selected_train_clics['user_id'].map(user_mapping.set_index('old').new)\n",
    "picture_ids = selected_train_clics['picture_id'].map(picture_mapping.set_index('old').new)\n",
    "\n",
    "train_picture_user_clicks_matrix = sp.csr_matrix(\n",
    "    (np.tile(1, selected_train_clics.shape[0]),\n",
    "        (\n",
    "            picture_ids,\n",
    "            user_ids\n",
    "        )\n",
    "    ),\n",
    "#     shape=(picture_ids.max() + 1, user_ids.max() + 1),\n",
    "    shape=(len(picture_mapping_dict) + 1, len(user_mapping_dict) + 1),\n",
    "    dtype=np.float\n",
    ")\n",
    "train_picture_user_clicks_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_one_true = (\n",
    "    val_clicks[val_clicks['user_id'].isin(val1_users)].groupby('user_id')['picture_id'].apply(list).to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_one_popular = (\n",
    "    val_clicks[val_clicks['user_id'].isin(val1_users)]\n",
    "    .groupby('picture_id')['user_id']\n",
    "    .count()\n",
    "    .sort_values(ascending=False)[:TOP_N]\n",
    ").index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_one_pred_popular = dict()\n",
    "for user_id in list(val1_users):\n",
    "    _pred = val_one_popular.copy()\n",
    "    val_one_pred_popular[user_id] = _pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b2ddf63f5e4fad92176f0a90ed9833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8275), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Просмотры отсутствуют: 0, Предсказание отсутствует: 0, Предсказано: 8275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'map': 2.0391888025618255e-05,\n",
       "  'mean_ndcg': 0.0005757853858061449,\n",
       "  'mean_recall': 9.07763178511357e-05},\n",
       " {'map': 9.07763178511357e-05,\n",
       "  'mean_ndcg': 0.0013293051359516616,\n",
       "  'mean_recall': 9.07763178511357e-05})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics_dict(\n",
    "    predict_dict=val_one_pred_popular, \n",
    "    actual_dict=val_one_true, \n",
    "    current_predict_user_uids=val1_users,\n",
    "    k=TOP_N,\n",
    "    nonpredicted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item2Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1497835/1497835 [00:09<00:00, 155892.11it/s]\n"
     ]
    }
   ],
   "source": [
    "model = ItemItemRecommender(K=200)\n",
    "model.fit(train_picture_user_clicks_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_one_pred_item2item = dict()\n",
    "for user_id in list(val1_users):\n",
    "    user_index = user_mapping_dict[user_id]\n",
    "    _pred = model.recommend(user_index, train_picture_user_clicks_matrix.T)\n",
    "    _pred = [inv_picture_mapping_dict[x[0]] for x in _pred]\n",
    "    _pred.extend(val_one_popular[:TOP_N - len(_pred)])\n",
    "    \n",
    "    val_one_pred_item2item[user_id] = _pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df568f13cc34dea97aa44fedc3e59b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8275), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Просмотры отсутствуют: 0, Предсказание отсутствует: 0, Предсказано: 8275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'map': 6.347410050260148e-06,\n",
       "  'mean_ndcg': 0.000265134597088794,\n",
       "  'mean_recall': 4.487928101754297e-05},\n",
       " {'map': 4.487928101754297e-05,\n",
       "  'mean_ndcg': 0.0008459214501510574,\n",
       "  'mean_recall': 4.487928101754297e-05})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics_dict(\n",
    "    predict_dict=val_one_pred_item2item, \n",
    "    actual_dict=val_one_true, \n",
    "    current_predict_user_uids=val1_users,\n",
    "    k=TOP_N,\n",
    "    nonpredicted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100.0/100 [01:32<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "# als_model = AlternatingLeastSquares(factors=64, iterations=100, use_gpu=False, regularization=0.01)\n",
    "als_model = AlternatingLeastSquares(factors=192, iterations=100, use_gpu=True, regularization=0.01)\n",
    "als_model.fit(train_picture_user_clicks_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9680966767371602 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    len(val1_users & set(train_clicks['user_id'].unique().tolist())) / len(val1_users),\n",
    "#     len(val1_users & set(cutted_train_clicks['user_id'].unique().tolist())) / len(val1_users),\n",
    "    len(val1_users & set(val_one_true.keys())) / len(val1_users),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa647e66b23a4cc8be2b3c868d2aeb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8275), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_one_pred_als = dict()\n",
    "\n",
    "for user_id in tqdm(list(val1_users)):\n",
    "    user_index = user_mapping_dict[user_id]\n",
    "    \n",
    "    if als_model.user_factors[user_index][0] == 0:\n",
    "        continue\n",
    "    \n",
    "    _pred = als_model.recommend(user_index, train_picture_user_clicks_matrix.T, TOP_N)\n",
    "    \n",
    "    _pred = [inv_picture_mapping_dict[x[0]] for x in _pred]\n",
    "    \n",
    "    val_one_pred_als[user_id] = _pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe40e4d17b724003b38026cd532c922f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8275), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Просмотры отсутствуют: 0, Предсказание отсутствует: 264, Предсказано: 8011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'map': 0.000974841883227078,\n",
       "  'mean_ndcg': 0.0035204697905248192,\n",
       "  'mean_recall': 0.0015133894462884255},\n",
       " {'map': 0.0015133894462884255,\n",
       "  'mean_ndcg': 0.005742104606166521,\n",
       "  'mean_recall': 0.0015133894462884255})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics_dict(\n",
    "    predict_dict=val_one_pred_als, \n",
    "    actual_dict=val_one_true, \n",
    "    current_predict_user_uids=val1_users,\n",
    "    k=TOP_N,\n",
    "    nonpredicted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ({'map': 0.0006754847755774471,\n",
    "#   'mean_ndcg': 0.002348070146136011,\n",
    "#   'mean_recall': 0.0008974340168676217},\n",
    "#  {'map': 0.0008974340168676216,\n",
    "#   'mean_ndcg': 0.0036200224691049806,\n",
    "#   'mean_recall': 0.0008974340168676216})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ({'map': 0.000978976469498359,\n",
    "#   'mean_ndcg': 0.00336726540704797,\n",
    "#   'mean_recall': 0.001473669945827444},\n",
    "#  {'map': 0.0014736699458274443,\n",
    "#   'mean_ndcg': 0.005367619523155661,\n",
    "#   'mean_recall': 0.0014736699458274443})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(os.path.join(DATA_PATH, 'step3_clicks_als.pkl'), 'wb') as f:\n",
    "#     pickle.dump(als_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Picture features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1497835x1497835 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1497835 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dim = train_picture_user_clicks_matrix.shape[0]\n",
    "# _dim = len(picture_mapping_dict)\n",
    "clicks_picture_eye_features = sp.csr_matrix(\n",
    "    (\n",
    "            np.tile(1, _dim), \n",
    "            (\n",
    "                np.arange(_dim), \n",
    "                np.arange(_dim)\n",
    "            )\n",
    "    ),\n",
    "    shape=(_dim, _dim),\n",
    "    dtype=np.float\n",
    ")\n",
    "clicks_picture_eye_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_picture_descriptions = pd.read_csv(os.path.join(RAW_DATA_PATH, 'descriptions.csv'))\n",
    "raw_picture_descriptions.head()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(\n",
    "    input='content', \n",
    "    encoding='utf-8',\n",
    "    decode_error='strict', \n",
    "    strip_accents=None, \n",
    "    lowercase=True,\n",
    "    preprocessor=None, \n",
    "    tokenizer=None, \n",
    "    analyzer='word',\n",
    "    stop_words=None, \n",
    "    token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "    ngram_range=(1, 3), \n",
    "    max_df=1.0, \n",
    "    min_df=2,\n",
    "    max_features=1000, \n",
    "    vocabulary=None, \n",
    "    binary=False,\n",
    "    dtype=np.float32, \n",
    "    norm='l2', \n",
    "    use_idf=True, \n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=False\n",
    ")\n",
    "# picture_descriptions_embeds = tf.fit_transform(raw_picture_descriptions['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inv_picture_desc_dict = raw_picture_descriptions[['picture_id']].to_dict()['picture_id']\n",
    "picture_desc_dict = dict([(v, k) for k,v in _inv_picture_desc_dict.items()])\n",
    "\n",
    "def generate_picture_descs(pairs):\n",
    "    global raw_picture_descriptions\n",
    "    global picture_descriptions_embeds\n",
    "    global picture_desc_dict\n",
    "    \n",
    "    _pairs = pairs.merge(raw_picture_descriptions, on=['picture_id'], how='left').fillna('')\n",
    "    gb = _pairs.groupby('description')['description'].first()\n",
    "    p_dict = dict(zip(gb.values, range(len(gb))))\n",
    "    \n",
    "    selected_picture_descriptions_embeds_pairs = (\n",
    "        picture_descriptions_embeds[_pairs['description'].apply(lambda x: p_dict[x]).values, :]\n",
    "    )\n",
    "    \n",
    "    return selected_picture_descriptions_embeds_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_picture_desc_features = generate_picture_descs(train_clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2314748x1000 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 9697719 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_picture_desc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 2314748, expected 1497835.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-8b17ee0d4a95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     [\n\u001b[1;32m      3\u001b[0m         \u001b[0mclicks_picture_eye_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mclicks_picture_desc_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     ]\n\u001b[1;32m      6\u001b[0m ).tocsr()\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/sparse/construct.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \"\"\"\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/sparse/construct.py\u001b[0m in \u001b[0;36mbmat\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    584\u001b[0m                                                     \u001b[0mexp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbrow_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                                                     got=A.shape[0]))\n\u001b[0;32m--> 586\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbcol_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 2314748, expected 1497835."
     ]
    }
   ],
   "source": [
    "clicks_picture_full_features = sp.hstack(\n",
    "    [\n",
    "        clicks_picture_eye_features,\n",
    "        clicks_picture_desc_features\n",
    "    ]\n",
    ").tocsr()\n",
    "clicks_picture_full_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### User features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<446189x446189 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 446189 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dim = train_picture_user_clicks_matrix.shape[1]\n",
    "# _dim = len(user_mapping_dict)\n",
    "\n",
    "clicks_user_eye_features = sp.csr_matrix(\n",
    "    (\n",
    "            np.tile(1, _dim), \n",
    "            (\n",
    "                np.arange(_dim), \n",
    "                np.arange(_dim)\n",
    "            )\n",
    "    ),\n",
    "    shape=(_dim ,_dim ),\n",
    "    dtype=np.float\n",
    ")\n",
    "clicks_user_eye_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<446189x446189 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 446189 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_user_full_features = sp.hstack(\n",
    "    [\n",
    "        clicks_user_eye_features\n",
    "    ]\n",
    ").tocsr()\n",
    "clicks_user_full_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x7f14e2d8ba90>"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightfm import LightFM\n",
    "\n",
    "lightfm_model = LightFM(\n",
    "    no_components=200, # 10\n",
    "#     k=5,\n",
    "#     n=100, # 10\n",
    "    learning_schedule=\"adadelta\",\n",
    "    loss=\"warp\",\n",
    "    learning_rate=0.07,\n",
    "    rho=0.95,\n",
    "    epsilon=1e-6,\n",
    "    item_alpha=0, # 0.0\n",
    "    user_alpha=0,\n",
    "    max_sampled=100, # 10\n",
    "    random_state=RANDOM_STATE #\n",
    ")\n",
    "\n",
    "lightfm_model.fit(\n",
    "    train_picture_user_clicks_matrix.T, \n",
    "    epochs=20, \n",
    "    num_threads=8,\n",
    "    verbose=True,\n",
    "    item_features=clicks_picture_full_features,\n",
    "    user_features=clicks_user_full_features,\n",
    "#     sample_weight=_train_sample_weight_matrix.T.tocoo()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "\n",
    "def get_lightfm_predict_dict(\n",
    "        predict_user_uids: List[int],\n",
    "        user_uid_to_cat: Dict[int, int],\n",
    "        element_cat_to_uid: Dict[int, int],\n",
    "        model: LightFM,\n",
    "        item_features: sp.csr_matrix,\n",
    "        user_features: sp.csr_matrix,\n",
    "        filter_dict: Dict[int, Set[int]],\n",
    "        top_n: int\n",
    ") -> Dict[int, Tuple[List[int], List[np.float32]]]:\n",
    "    # Предсказания в виде плотной матрицы\n",
    "    new_predict_user_cats = []\n",
    "    # Список uids, которые у нас есть в индексе ПОЛНЫХ данных обучения\n",
    "    new_predict_user_uids = []\n",
    "    for user_uid in predict_user_uids:\n",
    "        user_cat = user_uid_to_cat.get(user_uid)\n",
    "        if user_cat is not None:\n",
    "            new_predict_user_uids.append(user_uid)\n",
    "            new_predict_user_cats.append(user_cat)\n",
    "\n",
    "    # Получаем вектора предсказаний для пользователя\n",
    "    predict_dict = {}\n",
    "\n",
    "#     _element_cats = np.array(range(0, len(element_cat_to_uid)))\n",
    "    _element_cats = np.array(range(0, len(element_cat_to_uid) + 1))\n",
    "\n",
    "    for user_cat, user_uid in tqdm(zip(new_predict_user_cats, new_predict_user_uids), total=len(new_predict_user_uids)):\n",
    "        filter_elements = filter_dict[user_uid]\n",
    "        max_needed_item_count = len(filter_elements) + top_n\n",
    "\n",
    "        _user_cats = np.repeat(user_cat, len(_element_cats))\n",
    "        _user_cats = [1]\n",
    "#         _element_cats = [1, 2]\n",
    "        user_vec = model.predict(\n",
    "            user_ids=_user_cats,\n",
    "            item_ids=_element_cats,\n",
    "            item_features=item_features,\n",
    "            user_features=user_features,\n",
    "            num_threads=0\n",
    "        )\n",
    "\n",
    "        # Ищем наибольшие индексы с помощью argpartition\n",
    "        user_unsorted_pred = np.argpartition(\n",
    "            -user_vec,\n",
    "            max_needed_item_count\n",
    "        )[: max_needed_item_count + 1]\n",
    "\n",
    "        # Сортируем только наибольшие индексы\n",
    "        user_sorted_pred = [y[0] for y in sorted(\n",
    "            list(\n",
    "                zip(\n",
    "                    user_unsorted_pred,\n",
    "                    user_vec[user_unsorted_pred]\n",
    "                )), key=lambda x: -x[1]\n",
    "        )]\n",
    "\n",
    "        user_filtered_final_pred = []\n",
    "        user_filtered_final_pred_weights = []\n",
    "\n",
    "        for element_cat in user_sorted_pred:\n",
    "            if element_cat not in filter_elements:\n",
    "                user_filtered_final_pred_weights.append(user_vec[element_cat])\n",
    "                user_filtered_final_pred.append(element_cat_to_uid.get(element_cat))\n",
    "\n",
    "            if len(user_filtered_final_pred) == top_n:\n",
    "                break\n",
    "\n",
    "        predict_dict[user_uid] = (user_filtered_final_pred, user_filtered_final_pred_weights)\n",
    "\n",
    "    return predict_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f6f8d967d74547ae52dbba9d7513c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8275), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "current_predict_user_uids = val1_users\n",
    "\n",
    "full_val_one_pred_lightfm = get_lightfm_predict_dict(\n",
    "#     list(current_predict_user_uids)[:100],\n",
    "    current_predict_user_uids,\n",
    "    user_mapping_dict,\n",
    "    inv_picture_mapping_dict,\n",
    "    lightfm_model,\n",
    "    item_features=clicks_picture_full_features,\n",
    "    user_features=clicks_user_full_features,\n",
    "    filter_dict=defaultdict(list),\n",
    "    top_n=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выкидываем веса для подачи в Evaluator\n",
    "val_one_pred_lightfm = {}\n",
    "for k, v in full_val_one_pred_lightfm.items():\n",
    "    val_one_pred_lightfm[k] = v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e419db0759a444f8e11f77f9130bcd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8275), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Просмотры отсутствуют: 0, Предсказание отсутствует: 0, Предсказано: 8275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'map': 6.240380659604701e-05,\n",
       "  'mean_ndcg': 0.000430302183004857,\n",
       "  'mean_recall': 0.00015449815374286673},\n",
       " {'map': 0.00015449815374286673,\n",
       "  'mean_ndcg': 0.0009667673716012085,\n",
       "  'mean_recall': 0.00015449815374286673})"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics_dict(\n",
    "    predict_dict=val_one_pred_lightfm, \n",
    "    actual_dict=val_one_true, \n",
    "    current_predict_user_uids=val1_users,\n",
    "    k=TOP_N,\n",
    "    nonpredicted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popular\n",
    "# ({'map': 2.0391888025618255e-05,\n",
    "#   'mean_ndcg': 0.0005757853858061449,\n",
    "#   'mean_recall': 9.07763178511357e-05},\n",
    "#  {'map': 9.07763178511357e-05,\n",
    "#   'mean_ndcg': 0.0013293051359516616,\n",
    "#   'mean_recall': 9.07763178511357e-05})\n",
    "\n",
    "# Item2Item + Popular\n",
    "# ({'map': 1.9532606006026568e-05,\n",
    "#   'mean_ndcg': 0.0005380947885125098,\n",
    "#   'mean_recall': 8.27199230877923e-05},\n",
    "#  {'map': 8.27199230877923e-05,\n",
    "#   'mean_ndcg': 0.0012084592145015106,\n",
    "#   'mean_recall': 8.27199230877923e-05})\n",
    "\n",
    "# ALS (GPU x32)\n",
    "# ({'map': 0.00029929270789502937,\n",
    "#   'mean_ndcg': 0.0013584427545974098,\n",
    "#   'mean_recall': 0.000578502283034005},\n",
    "#  {'map': 0.000578502283034005,\n",
    "#   'mean_ndcg': 0.0027794561933534743,\n",
    "#   'mean_recall': 0.000578502283034005})\n",
    "\n",
    "# ALS (СPU x10)\n",
    "# ({'map': 1.4473761406501112e-05,\n",
    "#   'mean_ndcg': 0.00024672182460939323,\n",
    "#   'mean_recall': 4.299331928525759e-05},\n",
    "#  {'map': 4.299331928525759e-05,\n",
    "#   'mean_ndcg': 0.0006042296072507553,\n",
    "#   'mean_recall': 4.299331928525759e-05})\n",
    "\n",
    "# ALS (GPU x48==x64)\n",
    "# ({'map': 0.0009478596897275725,\n",
    "#   'mean_ndcg': 0.0029713081448600725,\n",
    "#   'mean_recall': 0.0012379757400905436},\n",
    "#  {'map': 0.0012379757400905436,\n",
    "#   'mean_ndcg': 0.004350453172205438,\n",
    "#   'mean_recall': 0.0012379757400905436})\n",
    "\n",
    "# ALS (GPU x64)!\n",
    "# ({'map': 0.0009478596897275725,\n",
    "#   'mean_ndcg': 0.0029713081448600725,\n",
    "#   'mean_recall': 0.0012379757400905436},\n",
    "#  {'map': 0.0012379757400905436,\n",
    "#   'mean_ndcg': 0.004350453172205438,\n",
    "#   'mean_recall': 0.0012379757400905436})\n",
    "\n",
    "# ALS (CPU x64), 100 iter\n",
    "# ({'map': 0.000978976469498359,\n",
    "#   'mean_ndcg': 0.00336726540704797,\n",
    "#   'mean_recall': 0.001473669945827444},\n",
    "#  {'map': 0.0014736699458274443,\n",
    "#   'mean_ndcg': 0.005367619523155661,\n",
    "#   'mean_recall': 0.0014736699458274443})\n",
    "\n",
    "# ALS (CPU x96, 30iter)\n",
    "# ({'map': 0.0008789522700955309,\n",
    "#   'mean_ndcg': 0.003111409247274019,\n",
    "#   'mean_recall': 0.0012420469313687357},\n",
    "#  {'map': 0.0012420469313687357,\n",
    "#   'mean_ndcg': 0.004833836858006042,\n",
    "#   'mean_recall': 0.0012420469313687357})\n",
    "\n",
    "# ALS (GPU x96, 100iter)\n",
    "# ({'map': 0.0008687408695060607,\n",
    "#   'mean_ndcg': 0.003040543044411595,\n",
    "#   'mean_recall': 0.0012984416947121394},\n",
    "#  {'map': 0.0012984416947121394,\n",
    "#   'mean_ndcg': 0.004954682779456193,\n",
    "#   'mean_recall': 0.0012984416947121394})\n",
    "\n",
    "# LightFM (Default, no_components=20, epochs=10)\n",
    "# ({'map': 1.2449460992815006e-05,\n",
    "#   'mean_ndcg': 0.0002660486737075211,\n",
    "#   'mean_recall': 5.551605060224218e-05},\n",
    "#  {'map': 5.551605060224218e-05,\n",
    "#   'mean_ndcg': 0.0007250755287009063,\n",
    "#   'mean_recall': 5.551605060224218e-05})\n",
    "\n",
    "# LightFM (Default, no_components=64, epochs=20)\n",
    "# ({'map': 6.240380659604701e-05,\n",
    "#   'mean_ndcg': 0.000430302183004857,\n",
    "#   'mean_recall': 0.00015449815374286673},\n",
    "#  {'map': 0.00015449815374286673,\n",
    "#   'mean_ndcg': 0.0009667673716012085,\n",
    "#   'mean_recall': 0.00015449815374286673})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
